\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}

\title{System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
14/02/2025 & 1.0 & First release\\
\bottomrule
\end{tabularx}


\newpage

\tableofcontents

\listoftables
\wss{Remove this section if it isn't needed}

\listoffigures
\wss{Remove this section if it isn't needed}

\newpage

\section{Symbols, Abbreviations, and Acronyms}

All symbols, abbreviations and acronyms used here are the same as in the SRS document. Please refer to the relevant section of the SRS for definitions. 

\newpage

\pagenumbering{arabic}

A Verification and Validation (V\&V) plan is a comprehensive guide outlining the goals, scope, techniques, and standards for confirming and validating a software product. 
Verification ensures that the software aligns with specified requirements and design criteria, while validation confirms that it fulfills user needs and expectations. 
This plan is crucial for maintaining software, dependability, quality, and functionality, and for identifying and resolving issues before release.

The following document presents the V\&V plan for \progname. It begins with the general information in Section 2, followed by a detailed discussion of the methods to achieve these objectives in Section 3. Finally, Section 4 provides the test descriptions.

\section{General Information}

\subsection{Summary}

The software to be tested is \progname. It can be used to simulate a continuous cellular automaton, allowing simulation parameters to be modified on the fly. 
In this way, the user can assess the effect of his modifications on the cell automaton, its evolution and its environment.

\subsection{Objectives}

The aim is to check the accuracy of the software, in the sense that the modifications applied to the simulation must be faithful to the modifications made by the user. 
Also, the graphical representation of the functions derived from the input parameters must be correct. 
For example, if the user wants the simulation to run twice as fast, then the simulation must run twice as fast.
If the user wants to add a diffusion constraint to the automaton, this constraint must be taken into account.

The quality of the code will also be assessed using static analysis such as linter.
The software depends on a graphics library, which will be assumed to be correct for the rest of the project. No checks will therefore be made on this library.

\subsection{Challenge Level and Extras}

The level of difficulty is admittedly intermediate, as a few mathematical models are involved in developing an unpredictable simulation.
 Also, particular attention will be paid to the user interface, which increases the complexity of the project.

\subsection{Relevant Documentation}

// TODO 

\wss{Reference relevant documentation.  This will definitely include your SRS
  and your other project documents (design documents, like MG, MIS, etc).  You
  can include these even before they are written, since by the time the project
  is done, they will be written.  You can create BibTeX entries for your
  documents and within those entries include a hyperlink to the documents.}

\citet{SRS}


\wss{Don't just list the other documents.  You should explain why they are relevant and 
how they relate to your VnV efforts.}

\section{Plan}

This section outlines the plan for verifying the documentation and software for \progname. 
The key elements to be verified include are SRS, design, V\&V, and implementation.

\subsection{SRS Verification Plan}

The SRS will be verified via feedback from a domain expert by using the SRS checklist designed by Dr.Smith. The feedback will be collected by an issue tracker.
The author will be responsible for dealing with each issue by discussing it with the sender and applying a corrective measure or justifying its absence.

\subsection{Design Verification Plan}

\wss{Plans for design verification}

\wss{The review will include reviews by your classmates}

\wss{Create a checklists?}

\subsection{Verification and Validation Plan Verification Plan}

The V\&V Plan will be verified via feedback from a domain expert by using the V\&V checklist designed by Dr.Smith. The feedback will be collected by an issue tracker.
The author will be responsible for dealing with each issue by discussing it with the sender and applying a corrective measure or justifying its absence.

\subsection{Implementation Verification Plan}

Verification of the implementation will be carried out by manual review and application of the unit tests listed below. 
Their results will be compared with those expected and potential differences will be justified.
Automatic tests will also be used.

\subsection{Automated Testing and Verification Tools}

The automatic tests will be carried out mainly by the pylint tool, which enables static verification of the code, in particular to check that it complies with the PEP8 python programming standard.
Pylint can also be used to check good code practices such as function atomicity and execution consistency, and can be used to optimise performance.
The use of pylint will be incorporated into the project's workflow and continuous integration.

\subsection{Software Validation Plan}

As part of this project, the software will be verified by comparing the resulting simulations with those of other continuous cellular automaton simulation software.

\section{System Tests}

The following section presents the tests to which the functional and non-functional requirements will be subjected, as well as a traceability matrix.

\subsection{Tests for Functional Requirements}

\wss{Subsets of the tests may be in related, so this section is divided into
  different areas.  If there are no identifiable subsets for the tests, this
  level of document structure can be removed.}

\wss{Include a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good here.}

\subsubsection{Area of Testing1}

\wss{It would be nice to have a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good here.  If a section
  covers tests for input constraints, you should reference the data constraints
  table in the SRS.}
		
\paragraph{Title for Test}

\begin{enumerate}

\item{test-id1\\}

Control: Manual versus Automatic
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs.  Output is not how you
are going to return the results of the test.  The output is the expected
result.}

Test Case Derivation: \wss{Justify the expected value given in the Output field}
					
How test will be performed: 
					
\item{test-id2\\}

Control: Manual versus Automatic
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\end{enumerate}

\subsubsection{Area of Testing2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{The nonfunctional requirements for accuracy will likely just reference the
  appropriate functional tests from above.  The test cases should mention
  reporting the relative error for these tests.  Not all projects will
  necessarily have nonfunctional requirements related to accuracy.}

\wss{For some nonfunctional tests, you won't be setting a target threshold for
passing the test, but rather describing the experiment you will do to measure
the quality for different inputs.  For instance, you could measure speed versus
the problem size.  The output of the test isn't pass/fail, but rather a summary
table or graph.}

\wss{Tests related to usability could include conducting a usability test and
  survey.  The survey will be in the Appendix.}

\wss{Static tests, review, inspections, and walkthroughs, will not follow the
format for the tests given below.}

\wss{If you introduce static tests in your plan, you need to provide details.
How will they be done?  In cases like code (or document) walkthroughs, who will
be involved? Be specific.}

\subsubsection{Area of Testing1}
		
\paragraph{Title for Test}

\begin{enumerate}

\item{test-id1\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Area of Testing2}

...

\subsection{Traceability Between Test Cases and Requirements}

\wss{Provide a table that shows which test cases are supporting which
  requirements.}

\section{Unit Test Description}

\wss{This section should not be filled in until after the MIS (detailed design
  document) has been completed.}

\wss{Reference your MIS (detailed design document) and explain your overall
philosophy for test case selection.}  

\wss{To save space and time, it may be an option to provide less detail in this section.  
For the unit tests you can potentially layout your testing strategy here.  That is, you 
can explain how tests will be selected for each module.  For instance, your test building 
approach could be test cases for each access program, including one test for normal behaviour 
and as many tests as needed for edge cases.  Rather than create the details of the input 
and output here, you could point to the unit testing code.  For this to work, you code 
needs to be well-documented, with meaningful names for all of the tests.}

\subsection{Unit Testing Scope}

As previously stated, the graphics library is assumed to be correct. Thus, no unit tests will be imposed on it. Unit tests are therefore limited to what the author has implemented.

\subsection{Tests for Functional Requirements}

\wss{Most of the verification will be through automated unit testing.  If
  appropriate specific modules can be verified by a non-testing based
  technique.  That can also be documented in this section.}

\subsubsection{Module 1}

\wss{Include a blurb here to explain why the subsections below cover the module.
  References to the MIS would be good.  You will want tests from a black box
  perspective and from a white box perspective.  Explain to the reader how the
  tests were selected.}

\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 
					
\item{test-id2\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\item{...\\}
    
\end{enumerate}

\subsubsection{Module 2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{If there is a module that needs to be independently assessed for
  performance, those test cases can go here.  In some projects, planning for
  nonfunctional tests of units will not be that relevant.}

\wss{These tests may involve collecting performance data from previously
  mentioned functional tests.}

\subsubsection{Module ?}
		
\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Module ?}

...

\subsection{Traceability Between Test Cases and Modules}

\wss{Provide evidence that all of the modules have been considered.}
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\subsection{Usability Survey Questions?}

\wss{This is a section that would be appropriate for some projects.}


\end{document}